{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimplestLoss-TopKPre.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba3c337873124472a1ef9cb8c5d36a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6d3cc1566984bfebcaa8483a6d26064",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e32760c4c3f540388ecabf5e9e9ba5aa",
              "IPY_MODEL_b69ffa422ebb4f1caa468eadd0d7c50e"
            ]
          }
        },
        "b6d3cc1566984bfebcaa8483a6d26064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e32760c4c3f540388ecabf5e9e9ba5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f45928d79a942c7974b83b43a211511",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_022a5fa3aeb544988133b67eceecfad8"
          }
        },
        "b69ffa422ebb4f1caa468eadd0d7c50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_62c1f537e7954d7f9d7bd7975a8a5710",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 167MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9aad20deaa5f498e8807e62efcdeb162"
          }
        },
        "6f45928d79a942c7974b83b43a211511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "022a5fa3aeb544988133b67eceecfad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62c1f537e7954d7f9d7bd7975a8a5710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9aad20deaa5f498e8807e62efcdeb162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c50434248db2465da9078877a8b3e449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4813273e681c4d618404bb9d82824467",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_86b7686b7664479d9e135856bf3e5512",
              "IPY_MODEL_6d2b5264f46f4421932be8c735ff7330"
            ]
          }
        },
        "4813273e681c4d618404bb9d82824467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86b7686b7664479d9e135856bf3e5512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5bb690752ee44787b9e378ebad15e927",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f33dd50f1b2e449b9eeb89e6da93e1d9"
          }
        },
        "6d2b5264f46f4421932be8c735ff7330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8eebe36a6822473586025f91c3e94f87",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 105735899.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b55079138fb4d6681310f6d01edaed6"
          }
        },
        "5bb690752ee44787b9e378ebad15e927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f33dd50f1b2e449b9eeb89e6da93e1d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8eebe36a6822473586025f91c3e94f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b55079138fb4d6681310f6d01edaed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgoySD7xMqAA"
      },
      "source": [
        "# DML loss function\n",
        "## 準備\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oALZvTmaMvgG",
        "outputId": "c0fa16dd-a7de-411a-8ec8-0e9b993ad324"
      },
      "source": [
        "!pip install -q pytorch-metric-learning[with-hooks]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 102kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 67.7MB 44kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKs4S44qMzLx",
        "outputId": "81ae04a1-854a-446f-9c77-fbc82384c3ff"
      },
      "source": [
        "%matplotlib inline\n",
        "from pytorch_metric_learning import losses, miners, samplers, trainers, testers\n",
        "from pytorch_metric_learning.utils import common_functions\n",
        "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "from cycler import cycler\n",
        "import record_keeper\n",
        "import pytorch_metric_learning\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "logging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:VERSION 0.9.96\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGPm0akqM3s4"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    # layer_sizes[0] is the dimension of the input\n",
        "    # layer_sizes[-1] is the dimension of the output\n",
        "    def __init__(self, layer_sizes, final_relu=False):\n",
        "        super().__init__()\n",
        "        layer_list = []\n",
        "        layer_sizes = [int(x) for x in layer_sizes]\n",
        "        num_layers = len(layer_sizes) - 1\n",
        "        final_relu_layer = num_layers if final_relu else num_layers - 1\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            input_size = layer_sizes[i]\n",
        "            curr_size = layer_sizes[i + 1]\n",
        "            if i < final_relu_layer:\n",
        "                layer_list.append(nn.ReLU(inplace=False))\n",
        "            layer_list.append(nn.Linear(input_size, curr_size))\n",
        "        self.net = nn.Sequential(*layer_list)\n",
        "        self.last_linear = self.net[-1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "ba3c337873124472a1ef9cb8c5d36a36",
            "b6d3cc1566984bfebcaa8483a6d26064",
            "e32760c4c3f540388ecabf5e9e9ba5aa",
            "b69ffa422ebb4f1caa468eadd0d7c50e",
            "6f45928d79a942c7974b83b43a211511",
            "022a5fa3aeb544988133b67eceecfad8",
            "62c1f537e7954d7f9d7bd7975a8a5710",
            "9aad20deaa5f498e8807e62efcdeb162"
          ]
        },
        "id": "sJoqvQSRNiyX",
        "outputId": "3ac39e5d-2e39-42f4-8283-cd2de7178561"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set trunk model and replace the softmax layer with an identity function\n",
        "trunk = torchvision.models.resnet18(pretrained=True)\n",
        "trunk_output_size = trunk.fc.in_features\n",
        "trunk.fc = common_functions.Identity()\n",
        "trunk = torch.nn.DataParallel(trunk.to(device))\n",
        "\n",
        "# Set embedder model. This takes in the output of the trunk and outputs 64 dimensional embeddings\n",
        "embedder = torch.nn.DataParallel(MLP([trunk_output_size, 64]).to(device))\n",
        "\n",
        "# Set optimizers\n",
        "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.0001)\n",
        "embedder_optimizer = torch.optim.Adam(embedder.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "\n",
        "# Set the image transforms\n",
        "train_transform = transforms.Compose([transforms.Resize(64),\n",
        "                                    transforms.RandomResizedCrop(scale=(0.16, 1), ratio=(0.75, 1.33), size=64),\n",
        "                                    transforms.RandomHorizontalFlip(0.5),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "val_transform = transforms.Compose([transforms.Resize(64),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba3c337873124472a1ef9cb8c5d36a36",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "c50434248db2465da9078877a8b3e449",
            "4813273e681c4d618404bb9d82824467",
            "86b7686b7664479d9e135856bf3e5512",
            "6d2b5264f46f4421932be8c735ff7330",
            "5bb690752ee44787b9e378ebad15e927",
            "f33dd50f1b2e449b9eeb89e6da93e1d9",
            "8eebe36a6822473586025f91c3e94f87",
            "1b55079138fb4d6681310f6d01edaed6"
          ]
        },
        "id": "TEXg0TsCNrGl",
        "outputId": "be04b2d1-3802-423a-9548-cbaa40cff6c7"
      },
      "source": [
        "# Download the original datasets\n",
        "original_train = datasets.CIFAR100(root=\"CIFAR100_Dataset\", train=True, transform=None, download=True)\n",
        "original_val = datasets.CIFAR100(root=\"CIFAR100_Dataset\", train=False, transform=None, download=True)\n",
        "\n",
        "# This will be used to create train and val sets that are class-disjoint\n",
        "class ClassDisjointCIFAR100(torch.utils.data.Dataset):\n",
        "    def __init__(self, original_train, original_val, train, transform):\n",
        "        rule = (lambda x: x < 50) if train else (lambda x: x >=50)\n",
        "        train_filtered_idx = [i for i,x in enumerate(original_train.targets) if rule(x)]\n",
        "        val_filtered_idx = [i for i,x in enumerate(original_val.targets) if rule(x)]\n",
        "        self.data = np.concatenate([original_train.data[train_filtered_idx], original_val.data[val_filtered_idx]], axis=0)\n",
        "        self.targets = np.concatenate([np.array(original_train.targets)[train_filtered_idx], np.array(original_val.targets)[val_filtered_idx]], axis=0)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, index):            \n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "# Class disjoint training and validation set\n",
        "train_dataset = ClassDisjointCIFAR100(original_train, original_val, True, train_transform)\n",
        "val_dataset = ClassDisjointCIFAR100(original_train, original_val, False, val_transform)\n",
        "assert set(train_dataset.targets).isdisjoint(set(val_dataset.targets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to CIFAR100_Dataset/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c50434248db2465da9078877a8b3e449",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting CIFAR100_Dataset/cifar-100-python.tar.gz to CIFAR100_Dataset\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wePXh6irNy0c"
      },
      "source": [
        "## Define the simplest loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCYdImpfN4XZ"
      },
      "source": [
        "from pytorch_metric_learning.losses import BaseMetricLossFunction\n",
        "import torch\n",
        "\n",
        "class BarebonesLoss(BaseMetricLossFunction):\n",
        "    def compute_loss(self, embeddings, labels, indices_tuple):\n",
        "        # perform some calculation #\n",
        "        print('indices tuple:', indices_tuple)\n",
        "        some_loss = torch.mean(embeddings)\n",
        "\n",
        "        # put into dictionary #\n",
        "        return {\n",
        "            \"loss\": {\n",
        "                \"losses\": some_loss,\n",
        "                \"indices\": None,\n",
        "                \"reduction_type\": \"already_reduced\",\n",
        "            }\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZedX1UDklB-m"
      },
      "source": [
        "# TopK-Pre\n",
        "## libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2cFg0dYlwcB"
      },
      "source": [
        "#####################\n",
        "# Global Attributes\n",
        "#####################\n",
        "\n",
        "\"\"\" Seed \"\"\"\n",
        "l2r_seed = 137\n",
        "\n",
        "\"\"\" A Small Value \"\"\"\n",
        "epsilon  = 1e-8\n",
        "\n",
        "\n",
        "\"\"\" GPU Setting If Expected \"\"\"\n",
        "\n",
        "#global_gpu, global_device, gpu_id = False, 'cpu', None\n",
        "global_gpu, global_device, gpu_id = True, 'cuda:0', 0\n",
        "#global_gpu, global_device, gpu_id = True, 'cuda:1', 1\n",
        "\n",
        "#\n",
        "if global_gpu: torch.cuda.set_device(gpu_id)\n",
        "\n",
        "# a uniform tensor type\n",
        "tensor      = torch.cuda.FloatTensor if global_gpu else torch.FloatTensor\n",
        "byte_tensor = torch.cuda.ByteTensor  if global_gpu else torch.ByteTensor\n",
        "long_tensor = torch.cuda.LongTensor  if global_gpu else torch.LongTensor\n",
        "\n",
        "# uniform constants\n",
        "torch_one, torch_half, torch_zero = tensor([1.0]), tensor([0.5]), tensor([0.0])\n",
        "torch_two = tensor([2.0])\n",
        "\n",
        "torch_minus_one = tensor([-1.0])\n",
        "\n",
        "def get_pairwise_stds(batch_labels):\n",
        "\t\"\"\"\n",
        "\t:param batch_labels: [batch_size], for each element of the batch assigns a class [0,...,C-1]\n",
        "\t:return: [batch_size, batch_size], where S_ij represents whether item-i and item-j belong to the same class\n",
        "\t\"\"\"\n",
        "\tassert 1 == len(batch_labels.size())\n",
        "\t#print(batch_labels.size())\n",
        "\tbatch_labels = batch_labels.type(tensor)\n",
        "\tcmp_mat = torch.unsqueeze(batch_labels, dim=1) - torch.unsqueeze(batch_labels, dim=0)\n",
        "\tsim_mat_std = torch.where(cmp_mat==0, torch_one, torch_zero)\n",
        "\n",
        "\treturn sim_mat_std\n",
        "\n",
        "def get_pairwise_similarity(batch_reprs):\n",
        "\t'''\n",
        "\ttodo-as-note Currently, it the dot-product of a pair of representation vectors, on the assumption that the input vectors are already normalized\n",
        "\tEfficient function to compute the pairwise similarity matrix given the input vector representations.\n",
        "\t:param batch_reprs: [batch_size, length of vector repr] a batch of vector representations\n",
        "\t:return: [batch_size, batch_size]\n",
        "\t'''\n",
        "\n",
        "\tsim_mat = torch.matmul(batch_reprs, batch_reprs.t())\n",
        "\treturn sim_mat\n",
        "\n",
        "def dist(batch_reprs, eps = 1e-16, squared=False):\n",
        "\t\"\"\"\n",
        "\tEfficient function to compute the distance matrix for a matrix A.\n",
        "\n",
        "\tArgs:\n",
        "\t\tbatch_reprs:  vector representations\n",
        "\t\teps: float, minimal distance/clampling value to ensure no zero values.\n",
        "\tReturns:\n",
        "\t\tdistance_matrix, clamped to ensure no zero values are passed.\n",
        "\t\"\"\"\n",
        "\tprod = torch.mm(batch_reprs, batch_reprs.t())\n",
        "\tnorm = prod.diag().unsqueeze(1).expand_as(prod)\n",
        "\tres = (norm + norm.t() - 2 * prod).clamp(min = 0)\n",
        "\n",
        "\tif squared:\n",
        "\t\treturn res.clamp(min=eps)\n",
        "\telse:\n",
        "\t\treturn res.clamp(min = eps).sqrt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYOM4RO_Gy16"
      },
      "source": [
        "## TopKPreMiner\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k-aENIyG4Oa"
      },
      "source": [
        "from pytorch_metric_learning.miners.base_miner import BaseTupleMiner\n",
        "from pytorch_metric_learning.distances import CosineSimilarity\n",
        "from pytorch_metric_learning.utils import common_functions as c_f\n",
        "from pytorch_metric_learning.utils import loss_and_miner_utils as lmu\n",
        "\n",
        "class TopKPreMiner(BaseTupleMiner):\n",
        "  def __init__(self, k, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.k = k\n",
        "        self.add_to_recordable_attributes(name=\"k\", is_stat=False)\n",
        "  \n",
        "  def mine(self, embeddings, labels, ref_emb, ref_labels):\n",
        "        print('embeddings', embeddings.size())\n",
        "        print('labels', labels.size())\n",
        "        print('ref_emb', ref_emb.size())\n",
        "        print('ref_labels', ref_labels.size())\n",
        "        mat = self.distance(embeddings, ref_emb)\n",
        "        a1, p, a2, n = lmu.get_all_pairs_indices(labels, ref_labels)\n",
        "\n",
        "        if len(a1) == 0 or len(a2) == 0:\n",
        "            empty = torch.LongTensor([]).to(labels.device)\n",
        "            return empty.clone(), empty.clone(), empty.clone(), empty.clone()\n",
        "\n",
        "        mat_neg_sorting = mat\n",
        "        mat_pos_sorting = mat.clone()\n",
        "\n",
        "        dtype = mat.dtype\n",
        "        pos_ignore = (\n",
        "            c_f.pos_inf(dtype) if self.distance.is_inverted else c_f.neg_inf(dtype)\n",
        "        )\n",
        "        neg_ignore = (\n",
        "            c_f.neg_inf(dtype) if self.distance.is_inverted else c_f.pos_inf(dtype)\n",
        "        )\n",
        "\n",
        "        mat_pos_sorting[a2, n] = pos_ignore\n",
        "        mat_neg_sorting[a1, p] = neg_ignore\n",
        "        if embeddings is ref_emb:\n",
        "            mat_pos_sorting.fill_diagonal_(pos_ignore)\n",
        "            mat_neg_sorting.fill_diagonal_(neg_ignore)\n",
        "\n",
        "        pos_sorted, pos_sorted_idx = torch.sort(mat_pos_sorting, dim=1)\n",
        "        neg_sorted, neg_sorted_idx = torch.sort(mat_neg_sorting, dim=1)\n",
        "\n",
        "        if self.distance.is_inverted:\n",
        "            hard_pos_idx = torch.where(\n",
        "                pos_sorted - self.epsilon < neg_sorted[:, -1].unsqueeze(1)\n",
        "            )\n",
        "            hard_neg_idx = torch.where(\n",
        "                neg_sorted + self.epsilon > pos_sorted[:, 0].unsqueeze(1)\n",
        "            )\n",
        "        else:\n",
        "            hard_pos_idx = torch.where(\n",
        "                pos_sorted + self.epsilon > neg_sorted[:, 0].unsqueeze(1)\n",
        "            )\n",
        "            hard_neg_idx = torch.where(\n",
        "                neg_sorted - self.epsilon < pos_sorted[:, -1].unsqueeze(1)\n",
        "            )\n",
        "\n",
        "        a1 = hard_pos_idx[0]\n",
        "        p = pos_sorted_idx[a1, hard_pos_idx[1]]\n",
        "        a2 = hard_neg_idx[0]\n",
        "        n = neg_sorted_idx[a2, hard_neg_idx[1]]\n",
        "\n",
        "        return a1, p, a2, n\n",
        "\n",
        "  def get_default_distance(self):\n",
        "      return CosineSimilarity()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-FEVFQUmvvo"
      },
      "source": [
        "## Define TopK-Pre Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LITW4txQlIjf"
      },
      "source": [
        "from pytorch_metric_learning.losses import BaseMetricLossFunction\n",
        "import torch\n",
        "\n",
        "class TopKPreLoss(BaseMetricLossFunction):\n",
        "    \"\"\"\n",
        "    Sampling Wisely: Deep Image Embedding by Top-K Precision Optimization\n",
        "    Jing Lu, Chaofan Xu, Wei Zhang, Ling-Yu Duan, Tao Mei; The IEEE International Conference on Computer Vision (ICCV), 2019, pp. 7961-7970\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k=4):#, anchor_id='Anchor', use_similarity=False, opt=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.name = 'TopKPreLoss'\n",
        "        # assert anchor_id in ANCHOR_ID\n",
        "\n",
        "        # self.opt = opt\n",
        "        # self.anchor_id = anchor_id\n",
        "        # self.use_similarity = use_similarity\n",
        "\n",
        "        self.k = 4\n",
        "        self.margin = 0.1 # self.opt.margin\n",
        "\n",
        "        # if 'Class' == anchor_id:\n",
        "        #     assert 0 == self.opt.bs % self.opt.samples_per_class\n",
        "        #     self.num_distinct_cls = int(self.opt.bs / self.opt.samples_per_class)\n",
        "\n",
        "    def compute_loss(self, embeddings, labels, indices_tuple): # (simi_mat, cls_match_mat, k=5, margin=None):\n",
        "        '''\n",
        "        assuming no-existence of classes with a single instance == samples_per_class > 1\n",
        "        :param sim_mat: [batch_size, batch_size] pairwise similarity matrix, without removing self-similarity\n",
        "        :param cls_match_mat: [batch_size, batch_size] v_ij is one if d_i and d_j are of the same class, zero otherwise\n",
        "        :param k: cutoff value\n",
        "        :param margin:\n",
        "        :return:\n",
        "        '''\n",
        "        # print('conpute loss')\n",
        "        # print('embeddings size', embeddings.size())\n",
        "        # print('labels size', labels.size())\n",
        "\n",
        "        simi_mat = get_pairwise_similarity(batch_reprs=embeddings)\n",
        "        cls_match_mat = get_pairwise_stds(\n",
        "            batch_labels=labels)  # [batch_size, batch_size] S_ij is one if d_i and d_j are of the same class, zero otherwise\n",
        "        # print('simi mat', simi_mat.size())\n",
        "        # print('class match mat', cls_match_mat.size())\n",
        "\n",
        "        simi_mat_hat = simi_mat + (1.0 - cls_match_mat) * self.margin  # impose margin\n",
        "\n",
        "        ''' get rank positions '''\n",
        "        _, orgp_indice = torch.sort(simi_mat_hat, dim=1, descending=True)\n",
        "        _, desc_indice = torch.sort(orgp_indice, dim=1, descending=False)\n",
        "        rank_mat = desc_indice + 1.  # todo using desc_indice directly without (+1) to improve efficiency\n",
        "        # print('rank_mat', rank_mat)\n",
        "\n",
        "        # number of true neighbours within the batch\n",
        "        batch_pos_nums = torch.sum(cls_match_mat, dim=1)\n",
        "\n",
        "        ''' get proper K rather than a rigid predefined K\n",
        "        torch.clamp(tensor, min=value) is cmax and torch.clamp(tensor, max=value) is cmin.\n",
        "        It works but is a little confusing at first.\n",
        "        '''\n",
        "        # batch_ks = torch.clamp(batch_pos_nums, max=k)\n",
        "        '''\n",
        "        due to no explicit self-similarity filtering.\n",
        "        implicit assumption: a common L2-normalization leading to self-similarity of the maximum one!\n",
        "        '''\n",
        "        batch_ks = torch.clamp(batch_pos_nums, max=self.k + 1)\n",
        "        k_mat = batch_ks.view(-1, 1).repeat(1, rank_mat.size(1))\n",
        "        # print('k_mat', k_mat.size())\n",
        "\n",
        "        '''\n",
        "        Only deal with a single case: n_{+}>=k\n",
        "        step-1: determine set of false positive neighbors, i.e., N, i.e., cls_match_std is zero && rank<=k\n",
        "\n",
        "        step-2: determine the size of N, i.e., |N| which determines the size of P\n",
        "\n",
        "        step-3: determine set of false negative neighbors, i.e., P, i.e., cls_match_std is one && rank>k && rank<= (k+|N|)\n",
        "        '''\n",
        "        # N\n",
        "        batch_false_pos = (cls_match_mat < 1) & (rank_mat <= k_mat)  # torch.uint8 -> used as indice\n",
        "        # print('batch_false_pos', batch_false_pos) bool\n",
        "        batch_fp_nums = torch.sum(batch_false_pos.float(), dim=1)  # used as one/zero\n",
        "        # print('batch_fp_nums', batch_fp_nums)\n",
        "\n",
        "        # P\n",
        "        batch_false_negs = cls_match_mat.bool() & (rank_mat > k_mat)  # all false negative\n",
        "\n",
        "        ''' just for check '''\n",
        "        # batch_fn_nums = torch.sum(batch_false_negs.float(), dim=1)\n",
        "        # print('batch_fn_nums', batch_fn_nums)\n",
        "\n",
        "        # batch_loss = 0\n",
        "        batch_loss = torch.tensor(0., requires_grad=True).cuda()\n",
        "        for i in range(cls_match_mat.size(0)):\n",
        "            fp_num = int(batch_fp_nums.data[i].item())\n",
        "            if fp_num > 0:  # error exists, in other words, skip correct case\n",
        "                # print('fp_num', fp_num)\n",
        "                all_false_neg = simi_mat_hat[i, :][batch_false_negs[i, :]]\n",
        "                # print('all_false_neg', all_false_neg)\n",
        "                top_false_neg, _ = torch.topk(all_false_neg, k=fp_num, sorted=False, largest=True)\n",
        "                # print('top_false_neg', top_false_neg)\n",
        "\n",
        "                false_pos = simi_mat_hat[i, :][batch_false_pos[i, :]]\n",
        "\n",
        "                loss = torch.sum(false_pos - top_false_neg)\n",
        "                batch_loss += loss\n",
        "        return {\n",
        "            \"loss\": {\n",
        "                \"losses\": batch_loss,\n",
        "                \"indices\": None,\n",
        "                \"reduction_type\": \"already_reduced\",\n",
        "            }\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRUVBsHSrV7h"
      },
      "source": [
        "## Define RS-TopK-Pre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYaCqRjLrp27"
      },
      "source": [
        "class RSTopKPreLoss(losses.BaseMetricLossFunction):\n",
        "    \"\"\"\n",
        "    Sampling Wisely: Deep Image Embedding by Top-K Precision Optimization\n",
        "    Jing Lu, Chaofan Xu, Wei Zhang, Ling-Yu Duan, Tao Mei; The IEEE International Conference on Computer Vision (ICCV), 2019, pp. 7961-7970\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k=4):#, anchor_id='Anchor'): # , use_similarity=False, opt=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.name = 'RSTopKPreLoss'\n",
        "\n",
        "        # self.opt = opt\n",
        "        # self.use_similarity = use_similarity\n",
        "\n",
        "        self.k = k\n",
        "        self.margin = 0.1 # self.opt.margin\n",
        "\n",
        "        # if 'Class' == anchor_id:置いておく\n",
        "        #     assert 0 == self.opt.bs % self.opt.samples_per_class\n",
        "        #     self.num_distinct_cls = int(self.opt.bs / self.opt.samples_per_class)\n",
        "\n",
        "    def compute_loss(self, embeddings, labels, indices_tuple): # (simi_mat, cls_match_mat, k=5, margin=None):\n",
        "        '''\n",
        "        assuming no-existence of classes with a single instance == samples_per_class > 1\n",
        "        :param sim_mat: [batch_size, batch_size] pairwise similarity matrix, without removing self-similarity\n",
        "        :param cls_match_mat: [batch_size, batch_size] v_ij is one if d_i and d_j are of the same class, zero otherwise\n",
        "        :param k: cutoff value\n",
        "        :param margin:\n",
        "        :return:\n",
        "        '''\n",
        "        # print('conpute loss')\n",
        "        # print('embeddings size', embeddings.size())\n",
        "        # print('labels size', labels.size())\n",
        "\n",
        "        simi_mat = get_pairwise_similarity(batch_reprs=embeddings)\n",
        "        cls_match_mat = get_pairwise_stds(\n",
        "            batch_labels=labels)  # [batch_size, batch_size] S_ij is one if d_i and d_j are of the same class, zero otherwise\n",
        "        # print('simi mat', simi_mat.size())\n",
        "        # print('class match mat', cls_match_mat.size())\n",
        "\n",
        "        simi_mat_hat = simi_mat + (1.0 - cls_match_mat) * self.margin  # impose margin\n",
        "\n",
        "        ''' get rank positions '''\n",
        "        _, orgp_indice = torch.sort(simi_mat_hat, dim=1, descending=True)\n",
        "        _, desc_indice = torch.sort(orgp_indice, dim=1, descending=False)\n",
        "        rank_mat = desc_indice + 1.  # todo using desc_indice directly without (+1) to improve efficiency\n",
        "        # print('rank_mat', rank_mat)\n",
        "\n",
        "        # number of true neighbours within the batch\n",
        "        batch_pos_nums = torch.sum(cls_match_mat, dim=1)\n",
        "\n",
        "        ''' get proper K rather than a rigid predefined K\n",
        "        torch.clamp(tensor, min=value) is cmax and torch.clamp(tensor, max=value) is cmin.\n",
        "        It works but is a little confusing at first.\n",
        "        '''\n",
        "        # batch_ks = torch.clamp(batch_pos_nums, max=k)\n",
        "        '''\n",
        "        due to no explicit self-similarity filtering.\n",
        "        implicit assumption: a common L2-normalization leading to self-similarity of the maximum one!\n",
        "        '''\n",
        "        batch_ks = torch.clamp(batch_pos_nums, max=self.k + 1)\n",
        "        k_mat = batch_ks.view(-1, 1).repeat(1, rank_mat.size(1))\n",
        "        # print('k_mat', k_mat.size())\n",
        "\n",
        "        '''\n",
        "        Only deal with a single case: n_{+}>=k\n",
        "        step-1: determine set of false positive neighbors, i.e., N, i.e., cls_match_std is zero && rank<=k\n",
        "\n",
        "        step-2: determine the size of N, i.e., |N| which determines the size of P\n",
        "\n",
        "        step-3: determine set of false negative neighbors, i.e., P, i.e., cls_match_std is one && rank>k && rank<= (k+|N|)\n",
        "        '''\n",
        "        # N\n",
        "        batch_false_pos = (cls_match_mat < 1) & (rank_mat <= k_mat)  # torch.uint8 -> used as indice\n",
        "        # print('batch_false_pos', batch_false_pos) bool\n",
        "        batch_fp_nums = torch.sum(batch_false_pos.float(), dim=1)  # used as one/zero\n",
        "        # print('batch_fp_nums', batch_fp_nums)\n",
        "\n",
        "        # P\n",
        "        batch_false_negs = cls_match_mat.bool() & (rank_mat > k_mat)  # all false negative\n",
        "\n",
        "        ''' just for check '''\n",
        "        # batch_fn_nums = torch.sum(batch_false_negs.float(), dim=1)\n",
        "        # print('batch_fn_nums', batch_fn_nums)\n",
        "\n",
        "        # batch_loss = 0\n",
        "        batch_loss = torch.tensor(0., requires_grad=True).cuda()\n",
        "        for i in range(cls_match_mat.size(0)):\n",
        "            fp_num = int(batch_fp_nums.data[i].item())\n",
        "            if fp_num > 0:  # error exists, in other words, skip correct case\n",
        "                all_false_neg = simi_mat_hat[i, :][batch_false_negs[i, :]]\n",
        "                rank_neg = rank_mat[i, :][batch_false_negs[i, :]]\n",
        "                top_false_neg, neg_idx = torch.topk(all_false_neg, k=fp_num, sorted=False, largest=True)\n",
        "                rank_top_neg = torch.gather(rank_neg, -1, neg_idx)\n",
        "                ks = torch.zeros(fp_num).cuda()\n",
        "                batch_ones = torch.ones_like(ks)\n",
        "                ks0 = ks.add(self.k)\n",
        "                # ks1 = ks.add(k+1)\n",
        "                ks3 = ks.add(self.k + 3)\n",
        "                beta1 = torch.add(batch_ones, -1 / (rank_top_neg - ks0))\n",
        "                # print('rank top neg', rank_top_neg)\n",
        "                # print('ks0', ks0)\n",
        "                # print('beta1', beta1)\n",
        "                # print('top_false_neg', top_false_neg)\n",
        "                loss_neg = 3 * torch.dot(beta1, top_false_neg) / fp_num\n",
        "                # print(\"loss_neg\", loss_neg)\n",
        "\n",
        "                rank_pos = rank_mat[i, :][batch_false_pos[i, :]]\n",
        "                false_pos = simi_mat_hat[i, :][batch_false_pos[i, :]]\n",
        "                # print('fp_num', fp_num)\n",
        "                # print('ks3', ks3)\n",
        "                # print('rank_pos', rank_pos)\n",
        "                # print('batch_ones', batch_ones)\n",
        "                beta2 = torch.add(batch_ones, -1 / (ks3 - rank_pos))\n",
        "                # print('beta2', beta2)\n",
        "                # print('false pos', false_pos)\n",
        "                # print('false_pos', false_pos)\n",
        "                loss_pos = 3 * torch.dot(beta2, false_pos) / fp_num\n",
        "                # print(\"loss_pos\", loss_pos)\n",
        "\n",
        "                loss = torch.sum(loss_pos - loss_neg)  # /fp_num\n",
        "                batch_loss += loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": {\n",
        "                \"losses\": batch_loss,\n",
        "                \"indices\": None,\n",
        "                \"reduction_type\": \"already_reduced\",\n",
        "            }\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zija1Ffhixxv"
      },
      "source": [
        "# L2R method for DML\n",
        "\n",
        "##  Lambdarank\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBeUWbNti2V5"
      },
      "source": [
        "def torch_ideal_dcg(batch_sorted_labels, gpu=False):\n",
        "    '''\n",
        "    :param sorted_labels: [batch, ranking_size]\n",
        "    :return: [batch, 1]\n",
        "    '''\n",
        "    batch_gains = torch.pow(2.0, batch_sorted_labels) - 1.0\n",
        "    batch_ranks = torch.arange(batch_sorted_labels.size(1))\n",
        "\n",
        "    batch_discounts = torch.log2(2.0 + batch_ranks.type(torch.cuda.FloatTensor)) if gpu else torch.log2(2.0 + batch_ranks.type(torch.FloatTensor))\n",
        "    batch_ideal_dcg = torch.sum(batch_gains / batch_discounts, dim=1, keepdim=True)\n",
        "\n",
        "    return batch_ideal_dcg\n",
        "\n",
        "\n",
        "def get_delta_ndcg(batch_stds, batch_stds_sorted_via_preds):\n",
        "    '''\n",
        "    Delta-nDCG w.r.t. pairwise swapping of the currently predicted ltr_adhoc\n",
        "    :param batch_stds: the standard labels sorted in a descending order\n",
        "    :param batch_stds_sorted_via_preds: the standard labels sorted based on the corresponding predictions\n",
        "    :return:\n",
        "    '''\n",
        "    batch_idcgs = torch_ideal_dcg(batch_sorted_labels=batch_stds, gpu=global_gpu)                      # ideal discount cumulative gains\n",
        "\n",
        "    batch_gains = torch.pow(2.0, batch_stds_sorted_via_preds) - 1.0\n",
        "    batch_n_gains = batch_gains / batch_idcgs               # normalised gains\n",
        "    batch_ng_diffs = torch.unsqueeze(batch_n_gains, dim=2) - torch.unsqueeze(batch_n_gains, dim=1)\n",
        "\n",
        "    batch_std_ranks = torch.arange(batch_stds_sorted_via_preds.size(1)).type(tensor)\n",
        "    batch_dists = 1.0 / torch.log2(batch_std_ranks + 2.0)   # discount co-efficients\n",
        "    batch_dists = torch.unsqueeze(batch_dists, dim=0)\n",
        "    batch_dists_diffs = torch.unsqueeze(batch_dists, dim=2) - torch.unsqueeze(batch_dists, dim=1)\n",
        "    batch_delta_ndcg = torch.abs(batch_ng_diffs) * torch.abs(batch_dists_diffs)  # absolute changes w.r.t. pairwise swapping\n",
        "\n",
        "    return batch_delta_ndcg\n",
        "\n",
        "\n",
        "def lambdarank_loss(batch_preds=None, batch_stds=None, sigma=1.0):\n",
        "    '''\n",
        "    This method will impose explicit bias to highly ranked documents that are essentially ties\n",
        "    :param batch_preds:\n",
        "    :param batch_stds:\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    batch_preds_sorted, batch_preds_sorted_inds = torch.sort(batch_preds, dim=1, descending=True)   # sort documents according to the predicted relevance\n",
        "    batch_stds_sorted_via_preds = torch.gather(batch_stds, dim=1, index=batch_preds_sorted_inds)    # reorder batch_stds correspondingly so as to make it consistent. BTW, batch_stds[batch_preds_sorted_inds] only works with 1-D tensor\n",
        "\n",
        "    batch_std_diffs = torch.unsqueeze(batch_stds_sorted_via_preds, dim=2) - torch.unsqueeze(batch_stds_sorted_via_preds, dim=1)  # standard pairwise differences, i.e., S_{ij}\n",
        "    batch_std_Sij = torch.clamp(batch_std_diffs, min=-1.0, max=1.0) # ensuring S_{ij} \\in {-1, 0, 1}\n",
        "\n",
        "    batch_pred_s_ij = torch.unsqueeze(batch_preds_sorted, dim=2) - torch.unsqueeze(batch_preds_sorted, dim=1)  # computing pairwise differences, i.e., s_i - s_j\n",
        "\n",
        "    batch_delta_ndcg = get_delta_ndcg(batch_stds, batch_stds_sorted_via_preds)\n",
        "    # print('batch_delta_ndcg', batch_delta_ndcg)\n",
        "\n",
        "    batch_loss_1st = 0.5 * sigma * batch_pred_s_ij * (1.0 - batch_std_Sij) # cf. the 1st equation in page-3\n",
        "    batch_loss_2nd = torch.log(torch.exp(-sigma * batch_pred_s_ij) + 1.0)  # cf. the 1st equation in page-3\n",
        "    # print('batch_loss_1st', batch_loss_1st)\n",
        "    # print('batch_loss_2nd', batch_loss_2nd)\n",
        "\n",
        "    # the coefficient of 0.5 is added due to all pairs are used\n",
        "    batch_loss = torch.sum(0.5 * (batch_loss_1st + batch_loss_2nd) * batch_delta_ndcg)    # weighting with delta-nDCG\n",
        "    # print('batch loss', batch_loss)\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq06b9mVj2uY"
      },
      "source": [
        "class Lambdarank(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Lambdarank, self).__init__()\n",
        "\n",
        "        self.name = 'lambdarank'\n",
        "        # assert anchor_id in ANCHOR_ID\n",
        "        # self.use_similarity = use_similarity\n",
        "\n",
        "    def forward(self, embeddings, labels, indices_tuple): #**kwargs\n",
        "        '''\n",
        "        :param batch_reprs:  torch.Tensor() [(BS x embed_dim)], batch of embeddings\n",
        "        :param batch_labels: [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        cls_match_mat = get_pairwise_stds(batch_labels=labels)  # [batch_size, batch_size] S_ij is one if d_i and d_j are of the same class, zero otherwise\n",
        "\n",
        "        # if self.use_similarity:\n",
        "        #     sim_mat = get_pairwise_similarity(batch_reprs=embeddings)\n",
        "        # else:\n",
        "        dist_mat = dist(batch_reprs=embeddings, squared=False)  # [batch_size, batch_size], pairwise distances\n",
        "        sim_mat = -dist_mat\n",
        "\n",
        "        # if 'Class' == self.anchor_id:  # vs. anchor wise sorting\n",
        "        #     cls_match_mat = cls_match_mat.view(self.num_distinct_cls, -1)\n",
        "        #     sim_mat = sim_mat.view(self.num_distinct_cls, -1)\n",
        "\n",
        "        # print('sim mat', sim_mat)\n",
        "        # print('cls_match mat', cls_match_mat)\n",
        "        batch_loss = lambdarank_loss(batch_preds=sim_mat, batch_stds=cls_match_mat)\n",
        "\n",
        "        return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zp0sSNIxR8w"
      },
      "source": [
        "## ListNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBCqivcuxV1X"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ListNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self): # , anchor_id='Anchor', use_similarity=False, opt=None\n",
        "        super(ListNet, self).__init__()\n",
        "\n",
        "        self.name = 'listnet'\n",
        "\n",
        "    def forward(self, embeddings, labels, indices_tuple):\n",
        "        '''\n",
        "        :param batch_reprs:  torch.Tensor() [(BS x embed_dim)], batch of embeddings\n",
        "        :param batch_labels: [(BS x 1)], for each element of the batch assigns a class [0,...,C-1]\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        cls_match_mat = get_pairwise_stds(batch_labels=labels)  # [batch_size, batch_size] S_ij is one if d_i and d_j are of the same class, zero otherwise\n",
        "\n",
        "        # if self.use_similarity:\n",
        "        #     sim_mat = get_pairwise_similarity(batch_reprs=batch)\n",
        "        # else:\n",
        "        dist_mat = dist(batch_reprs=embeddings, squared=False)  # [batch_size, batch_size], pairwise distances\n",
        "        sim_mat = -dist_mat\n",
        "\n",
        "        # convert to one-dimension vector\n",
        "        batch_size = embeddings.size(0)\n",
        "        index_mat = torch.triu(torch.ones(batch_size, batch_size), diagonal=1) == 1\n",
        "        sim_vec = sim_mat[index_mat]\n",
        "        cls_vec = cls_match_mat[index_mat]\n",
        "\n",
        "        # cross-entropy between two softmaxed vectors\n",
        "        # batch_loss = -torch.sum(F.softmax(sim_vec) * F.log_softmax(cls_vec))\n",
        "        batch_loss = -torch.sum(F.softmax(cls_vec) * F.log_softmax(sim_vec))\n",
        "\n",
        "        return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaZvZiW-N_fZ"
      },
      "source": [
        "## Create the loss, miner, sampler, and package them into dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAOdBWLjN9wd"
      },
      "source": [
        "# Set the loss function\n",
        "loss = TopKPreLoss(k=4)\n",
        "# loss = Lambdarank()\n",
        "# loss = ListNet()\n",
        "\n",
        "# Set the mining function\n",
        "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
        "miner = TopKPreMiner(k=4)\n",
        "\n",
        "# Set the dataloader sampler\n",
        "sampler = samplers.MPerClassSampler(train_dataset.targets, m=4, length_before_new_iter=len(train_dataset))\n",
        "\n",
        "# Set other training parameters\n",
        "batch_size = 32\n",
        "num_epochs = 2\n",
        "\n",
        "# Package the above stuff into dictionaries.\n",
        "models = {\"trunk\": trunk, \"embedder\": embedder}\n",
        "optimizers = {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}\n",
        "loss_funcs = {\"metric_loss\": loss}\n",
        "mining_funcs = {\"tuple_miner\": miner}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKI-gIXOpDY"
      },
      "source": [
        "record_keeper, _, _ = logging_presets.get_record_keeper(\"example_logs\", \"example_tensorboard\")\n",
        "hooks = logging_presets.get_hook_container(record_keeper)\n",
        "dataset_dict = {\"val\": val_dataset}\n",
        "model_folder = \"example_saved_models\"\n",
        "\n",
        "def visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n",
        "    logging.info(\"UMAP plot for the {} split and label set {}\".format(split_name, keyname))\n",
        "    label_set = np.unique(labels)\n",
        "    num_classes = len(label_set)\n",
        "    fig = plt.figure(figsize=(20,15))\n",
        "    plt.gca().set_prop_cycle(cycler(\"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]))\n",
        "    for i in range(num_classes):\n",
        "        idx = labels == label_set[i]\n",
        "        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)   \n",
        "    plt.show()\n",
        "\n",
        "# Create the tester\n",
        "tester = testers.GlobalEmbeddingSpaceTester(end_of_testing_hook = hooks.end_of_testing_hook, \n",
        "                                            visualizer = umap.UMAP(), \n",
        "                                            visualizer_hook = visualizer_hook,\n",
        "                                            dataloader_num_workers = 32)\n",
        "\n",
        "end_of_epoch_hook = hooks.end_of_epoch_hook(tester, \n",
        "                                            dataset_dict, \n",
        "                                            model_folder, \n",
        "                                            test_interval = 1,\n",
        "                                            patience = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbKh1hXyOrxM"
      },
      "source": [
        "trainer = trainers.MetricLossOnly(models,\n",
        "                                optimizers,\n",
        "                                batch_size,\n",
        "                                loss_funcs,\n",
        "                                mining_funcs,\n",
        "                                train_dataset,\n",
        "                                sampler=sampler,\n",
        "                                dataloader_num_workers = 32,\n",
        "                                end_of_iteration_hook = hooks.end_of_iteration_hook,\n",
        "                                end_of_epoch_hook = end_of_epoch_hook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2UtuLi3OvTm"
      },
      "source": [
        "trainer.train(num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBwB0TZbOyXH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}